{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a61dc7a-636d-4ec7-923c-f08ac69a0956",
   "metadata": {},
   "source": [
    "# \"[Recommender System] Factorization Machines\"\n",
    "> SVM과 Factorization Model의 장점을 합친 모델\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Paper]\n",
    "- tags: [paper, recommender system, factorization machines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d293365-a22c-4e84-9396-e2805c994db4",
   "metadata": {},
   "source": [
    "# Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07665ad9-0f44-4747-b221-2548cd0175cf",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9380e-250f-4f00-a4f1-6ce2fbe1649e",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c87755-d800-47a8-9b7f-b68b567c6c46",
   "metadata": {},
   "source": [
    "- **Factorization machine**은 **SVM**과 **Factorization Model**의 장점을 합친 모델\n",
    "    - **Factorization Model**의 예시: **Matrix Factorization**, **Parallel factor analysis**, **specialized model** (**SVD++**, **PITF** or **FPMC**)\n",
    "- Real valued Feature Vector를 활용한 General Predictor\n",
    "- Factorization Machine의 식은 linear time\n",
    "- 일반적인 추천 시스템은 special input data, optimization algorithm 등이 필요\n",
    "- 반면, Factorization Machine은 어느 곳이든 쉽게 적용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41584549-69f2-4ef9-a0ba-029a20b040ae",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246ded7-f6a9-44a3-8378-6068d845951f",
   "metadata": {},
   "source": [
    "- **Factorization Machine**은 general predictor이며, high sparsity에서도 reliable parameter를 예측할 수 있음\n",
    "- Sparse한 상황에서 **SVM**은 부적절\n",
    "    - Cannot learn reliable parameters(hyperplanes) in complex(non-linear) kernel spaces\n",
    "- FM은 복잡한 interaction도 모델링하고, factorized parameterization를 사용\n",
    "- Linear Complexity이며, linear number of parameters 임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f002a0-2d01-478c-8793-6916291be95d",
   "metadata": {},
   "source": [
    "## Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b23a7b-516d-46d5-96ad-cdaec80218c9",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fcdbfe-e948-486d-a5e8-6b4fd15e4115",
   "metadata": {},
   "source": [
    "- **Factorization Machine**의 장점\n",
    "    - Sparse data(일반적인 상황)\n",
    "    - Linear Complexity\n",
    "    - General Predictor로써 추천시스템 이외 다른 머신러닝에서 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1970c7-3b0e-445c-81fe-0f3a1b6439b0",
   "metadata": {},
   "source": [
    "## Prediction Under Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b9045-2dbe-471d-a3af-364dcd320e41",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569e564-f05d-44c8-9719-0c35f55c5a04",
   "metadata": {},
   "source": [
    "- 일반적으로 볼 수 있는 영화 평점 데이터의 설명\n",
    "- **Matrix Factorization**은 user와 movie, 그리고 해당 rating만 사용\n",
    "- **Factorization Machine**은 더 다양하게 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9e1fd-c13e-4cea-8960-292585bd48bd",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9ebc2-49de-4273-a0eb-75e68b0a63d5",
   "metadata": {},
   "source": [
    "## Factorization Machine (FM)\n",
    "### Model Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564248b-7acb-487f-be2f-a4ef8d488a57",
   "metadata": {},
   "source": [
    "$ \\Large \\displaystyle \\hat{y}(x) := w_{0} + \\sum_{i=1}^{n}w_{i}x_{i} + \\sum_{i=1}^{n} \\sum_{j=i+1}^{n}<v_{i},v_{j}>x_{i}x_{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ef3c1-88bc-450b-a21f-9256a8398ba1",
   "metadata": {},
   "source": [
    "> $w_{0}$ &larr; Global Bias  \n",
    "  \n",
    "> $w_{i}x_{i}$ &larr; $i$번째 weight를 모델링 함  \n",
    "  \n",
    "> $<v_{i}, v_{j}>$\n",
    "> - $<v_{i}, v_{j}> := \\displaystyle\\sum_{f=1}^{k}v_{i,f} \\cdot v_{j,f} $  \n",
    "> - Size $k$의 두 벡터를 내적 함 \n",
    "  \n",
    "> $\\displaystyle\\sum_{i=1}^{n}w_{i}x_{i}$  \n",
    "> - Matrix Factorization &larr; $W_{u} \\times W_{j} $  \n",
    ">     - user와 item latent vector  \n",
    "> - Factorization Machine  &larr; $W_{i} \\times x_{i} $\n",
    ">     - $x_{i}$마다 구함\n",
    "  \n",
    "> $<v_{i},v_{j}>x_{i}x_{y}$\n",
    "> - 변수간 latent vector 조합을 고려\n",
    "> - 이 때, degree = 2인 경우 모든 interaction을 얻을 수 있음\n",
    "> - pairwise feature interaction을 고려하기 때문에 sparse한 환경에 적합\n",
    "<br><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3a8d9-1ebf-43a7-9f7e-f1a0e921dc78",
   "metadata": {},
   "source": [
    "- $w_{0} \\in \\mathbb{R}, w \\in \\mathbb{R}^n, V \\in \\mathbb{R}^{n \\times k}$ &rarr; 추정해야 할 파라미터\n",
    "- 2-way FM(degree = 2): 개별 변수 또는 변수 간 interaction 모두 모델링 함\n",
    "- 다항 회귀와 매우 흡사하지만, coefficient 대신 파라미터마다 embedding vector를 만들어서 내적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b406ce2-d9c0-4148-9ebf-c667a08a166c",
   "metadata": {},
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce7919-504e-48f9-b078-3ef78051d50a",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2d772-1dab-4bd1-9b66-52de3e99c50e",
   "metadata": {},
   "source": [
    "- Factorization of pairwise interaction\n",
    "- 2개 변수에 직접적으로 연관있는 model parameter가 없음\n",
    "- Pairwise interaction 식을 정리하면 다음과 같음\n",
    "    - $O(kn^2)$ &rarr; $O(kn)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b609fe55-9ce7-4d80-bbee-0c6d404e8f49",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-006-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f8e96-4c69-444b-94fd-fe4def10c202",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-006-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7288b8-59b0-41c3-9518-b8d198f08227",
   "metadata": {},
   "source": [
    "### d-way Factorization machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b2990-67e6-4a53-b3e4-25fd8a656e40",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ffc7e-795e-4f24-8360-26b4f5bf5c6c",
   "metadata": {},
   "source": [
    "- 2-way FM을 d-way FM으로 일반화할 수 있음\n",
    "- 마찬가지로 computation cost는 linear임\n",
    "- [http://www.libfm.org](http://www.libfm.org) 에 다양하게 활용할 수 있는 module을 공개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79790e7-d360-44f6-b582-46e1281d7ad0",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fcede8-3e38-4436-8286-a298f7b2a31e",
   "metadata": {},
   "source": [
    "![](../images/paper/recommendation/fm/fm-009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce73ed0-3da1-412a-9613-0ca32408467e",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1dcb3-ec81-48e4-9417-d656b506231e",
   "metadata": {},
   "source": [
    "- Factorized Interaction을 사용하여 feature vector x의 모든 가능한 interaction을 모델링\n",
    "- (High) sparse한 상황에서 interaction을 추정할 수 있음. Unobserved interaction에 대해서도 일반화할 수 있음\n",
    "- 파라미터 수, 학습과 예측 시간 모두 linear 함 (Linear Complexity)\n",
    "- 이는 **SGD**를 활용한 최적화를 진행할 수 있고 다양한 loss function을 사용할 수 있음\n",
    "- **SVM**, **matrix**, **tensor and specialized factorization model** 보다 더 나은 성능을 증명"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
