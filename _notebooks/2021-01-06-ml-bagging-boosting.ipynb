{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"[ML] Bagging과 Boosting\"\n",
    "> 배깅과 부스팅의 개념\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [ML]\n",
    "- tags: [ml, bagging, boosting, randomforest, xgboost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Boosting 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편향-분산 상충관계(Bias-variance Trade-off)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) 편향과 분산의 정의**\n",
    "- **편향(Bias):** 점추정  \n",
    "    - 예측값과 실제값의 차이  \n",
    "    - 모델 학습시 여러 데이터로 학습 후 예측값의 범위가 정답과 얼마나 멀리 있는지 측정  \n",
    "- **편향(Bias(Real)):** 모형화(단순화)로 미처 반영하지 못한 복잡성  \n",
    "    <U>=> 편향이 작다면 Training 데이터 패턴(복잡성)을 최대반영 의미</U>  \n",
    "    <U>=> 편향이 크다면 Training 데이터 패턴(복잡성)을 최소반영 의미</U>  \n",
    "- **분산(Variance):** 구간추정  \n",
    "    - 학습한 모델의 예측값이 평균으로부터 퍼진 정도(변동성/분산)  \n",
    "    - 여러 모델로 학습을 반복한다면, 학습된 모델별로 예측한 값들의 차이를 측정\n",
    "- **분산(Variance(Real)):** 다른 데이터(Testing)를 사용했을때 발생할 변화  \n",
    "    <U>=> 분산이 작다면 다른 데이터로 예측시 적은 변동 예상</U>  \n",
    "    <U>=> 분산이 크다면 다른 데이터로 예측시 많은 변동 예상</U>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{align*}\n",
    "\\text{Equation of Error} && Err(x) &= E\\Bigl[\\bigl(Y-\\hat{f}(x)\\bigr)^2 \\Bigr] \\\\\n",
    "&& &= \\Bigl(E[\\hat{f}(x)] - f(x)\\Bigr)^2 + E \\Bigl[\\bigl(\\hat{f}(x) - E[\\hat{f}(x)]\\bigr)^2 \\Bigr] + \\sigma_{\\epsilon}^2 \\\\\n",
    "&& &= \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "\\end{align*} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) 편향과 분산의 관계**\n",
    "- **모델의 복잡도가 낮으면 Bias가 증가하고 Variance가 감소(Underfitting)**  \n",
    ": 구간추정 범위는 좁으나 점추정 정확성 낮음  \n",
    ": Training/Testing 모두 예측력이 낮음\n",
    "- **모델의 복잡도가 높으면 Bias가 감소하고 Variance가 증가(Overfitting)**  \n",
    ": 점추정 정확성은 높으나 구간추정 범위는 넓음  \n",
    ": Training만 잘 예측력 높고 Testing은 예측력 낮음  \n",
    "- **Bias와 Variance가 최소화 되는 수준에서 모델의 복잡도 선택**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging vs Boosting\n",
    "\n",
    "> **앙상블(Ensemble, Ensemble Learning, Ensemble Method)이란 머신러닝에서 여러개의 모델을 학습시켜,  \n",
    "그 모델들의 예측결과들을 이용해 하나의 모델보다 더 나은 값을 예측하는 방법**\n",
    "\n",
    "- **Bagging(Bootstrap Aggregating):**\n",
    "    - BootStrapping을 통해 여러 학습 데이터를 만들고, 이들을 averaging prediction을 통해 합침\n",
    "        - 부트스트래핑(Bootstraping): 예측값과 실제값의 차이 중복을 허용한 리샘플링(Resampling)  \n",
    "        - 페이스팅(Pasting): 이와 반대로 중복을 허용하지 않는 샘플링 \n",
    "    - Bias가 낮은 모델들을 이용해서 variance를 줄임\n",
    "    - **의사결정나무(Decision Tree):**\n",
    "    - **렌덤포레스트(Random Forest):** 여러개의 의사결정나무(Decision Tree) 생성한 다음, 각 개별 트리의 예측값들 중 가장 많은 선택을 받은 변수들로 예측하는 알고리즘, 의사결정나무의 CLT버전\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# DecisionTree\n",
    "fit = DecisionTreeRegressor().fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "\n",
    "# RandomForestRegressor\n",
    "fit = RandomForestRegressor(n_estimators=100, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Boosting:**   \n",
    "    - 성능이 약한 학습기(weak learner)를 여러 개 연결하여 강한 학습기(strong learner)를 만드는 앙상블 학습  \n",
    "    - 앞에서 학습된 모델을 보완해나가면서 더나은 모델로 학습시키는 것  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| - | Bagging | Boosting |\n",
    "|-------------|---------------------------------------|-----------------------------------------|\n",
    "| 특징 | 병렬 앙상블 모델(각 모델은 서로 독립) | 연속 앙상블 모델(이전 모델의 오류 반영) |\n",
    "| 목적 | Variance 감소 | Bias 감소 |\n",
    "| 적합한 상황 | Low Bias + High Variance | High Bias + Low Variance |\n",
    "| Sampling | Random Sampling | Random Sampling with weight on error |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting 알고리즘\n",
    "\n",
    "- **Adaptive Boosting(AdaBoost):** 학습된 모델이 과소적합(학습하기 어려운 데이터)된 학습 데이터 샘플의 가중치를 높이면서 더 잘 적합되도록 하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Boosting Machine(GBM):** 아다부스트 처럼 학습단계 마다 데이터 샘플의 가중치를 업데이트 하는 것이 아니라, 학습 전단계 모델에서의 잔차(Residual)을 모델에 학습시키는 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **XGBoost(eXtreme Gradient Boosting):** 높은 예측력으로 많은 양의 데이터를 다룰 때 사용되는 부스팅 알고리즘  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LightGBM:** 현존하는 부스팅 알고리즘 중 가장 빠르고 높은 예측력 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithms | Specification | Others |\n",
    "|------------|-------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n",
    "| AdaBoost | 다수결을 통한 정답분류 및 오답에 가중치 부여 | - |\n",
    "| GBM | 손실함수(검증지표)의 Gradient로 오답에 가중치 부여 | - |\n",
    "| XGBoost | GMB대비 성능향상<br/>시스템(CPU, Mem.) 자원 효율적 사용 | 2014년 공개 |\n",
    "| LightGBM | XGBoost대비 성능향상 및 자원소모 최소화<br/>XGBoost가 처리하지 못하는 대용량 데이터 학습가능<br/>근사치분할(Approximates the Split)을 통한 성능향상 | 2016년 공개 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# GradientBoostingRegression\n",
    "fit = GradientBoostingRegressor(alpha=0.1, learning_rate=0.05, loss='huber', criterion='friedman_mse',\n",
    "                                           n_estimators=1000, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "\n",
    "# XGBoost\n",
    "fit = XGBRegressor(learning_rate=0.05, n_estimators=100, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "\n",
    "# LightGMB\n",
    "fit = LGBMRegressor(learning_rate=0.05, n_estimators=100, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
