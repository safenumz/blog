{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"[Spark] PySpark 추천시스템 모델\"\n",
    "> pyspark 추천시스템 모델\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Spark]\n",
    "- tags: [spark, pyspark, recommendation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MINIO_ACCESS_KEY = os.environ['MINIO_ACCESS_KEY']\n",
    "MINIO_SECRET_KEY = os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.endpoint\", \"http://lab101:10170\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"com.amazonaws.services.s3.enableV2\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "ratings = spark.read.text(\"s3a://data/sample_movielens_ratings.txt\")\\\n",
    "    .rdd.toDF()\\\n",
    "    .selectExpr(\"split(value, '::') as col\")\\\n",
    "    .selectExpr(\n",
    "        \"cast(col[0] as int) as userId\",\n",
    "        \"cast(col[1] as int) as movieId\",\n",
    "        \"cast(col[2] as int) as rating\",\n",
    "        \"cast(col[3] as long) as timestamp\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     0|      2|     3|1424380312|\n",
      "|     0|      3|     1|1424380312|\n",
      "|     0|      5|     2|1424380312|\n",
      "|     0|      9|     4|1424380312|\n",
      "|     0|     11|     1|1424380312|\n",
      "|     0|     12|     2|1424380312|\n",
      "|     0|     15|     1|1424380312|\n",
      "|     0|     17|     1|1424380312|\n",
      "|     0|     19|     1|1424380312|\n",
      "|     0|     21|     1|1424380312|\n",
      "|     0|     23|     1|1424380312|\n",
      "|     0|     26|     3|1424380312|\n",
      "|     0|     27|     1|1424380312|\n",
      "|     0|     28|     1|1424380312|\n",
      "|     0|     29|     1|1424380312|\n",
      "|     0|     30|     1|1424380312|\n",
      "|     0|     31|     1|1424380312|\n",
      "|     0|     34|     1|1424380312|\n",
      "|     0|     37|     1|1424380312|\n",
      "|     0|     41|     2|1424380312|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "blockSize: block size for stacking input data in matrices. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. (default: 4096)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "coldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: 'nan', 'drop'. (default: nan)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: False)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: max number of iterations (>= 0). (default: 10, current: 5)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: False)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "rank: rank of the factorization (default: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0). (default: 0.1, current: 0.01)\n",
      "seed: random seed. (default: 3629070045491600340)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n"
     ]
    }
   ],
   "source": [
    "training, test = ratings.randomSplit([0.8, 0.2])\n",
    "als = ALS()\\\n",
    "    .setMaxIter(5)\\\n",
    "    .setRegParam(0.01)\\\n",
    "    .setUserCol(\"userId\")\\\n",
    "    .setItemCol(\"movieId\")\\\n",
    "    .setRatingCol(\"rating\")\n",
    "print(als.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsModel = als.fit(training)\n",
    "predictions = alsModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|            col|\n",
      "+------+---------------+\n",
      "|    20| {22, 4.846066}|\n",
      "|    20| {18, 4.623545}|\n",
      "|    20|{75, 4.0272512}|\n",
      "|    20|{27, 4.0242887}|\n",
      "|    20| {77, 3.781201}|\n",
      "|    20| {62, 3.671545}|\n",
      "|    20|{36, 3.2965648}|\n",
      "|    20|{74, 3.2775805}|\n",
      "|    20| {80, 3.175223}|\n",
      "|    20|{94, 3.1339927}|\n",
      "|    10| {74, 4.163344}|\n",
      "|    10|{87, 4.0253325}|\n",
      "|    10|  {2, 3.832425}|\n",
      "|    10|{53, 3.7631018}|\n",
      "|    10| {40, 3.762528}|\n",
      "|    10|{70, 3.1891506}|\n",
      "|    10| {59, 3.128613}|\n",
      "|    10|{42, 3.0709763}|\n",
      "|    10|{32, 3.0483675}|\n",
      "|    10|{49, 3.0007033}|\n",
      "+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alsModel.recommendForAllUsers(10)\\\n",
    "    .selectExpr(\"userId\", \"explode(recommendations)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|movieId|            col|\n",
      "+-------+---------------+\n",
      "|     20| {17, 4.662134}|\n",
      "|     20| {22, 4.212014}|\n",
      "|     20| {9, 3.8890233}|\n",
      "|     20|{23, 3.8233962}|\n",
      "|     20|{12, 3.7794924}|\n",
      "|     20|{29, 3.4634056}|\n",
      "|     20| {5, 3.2149444}|\n",
      "|     20|{10, 2.4653668}|\n",
      "|     20|{24, 2.3592012}|\n",
      "|     20| {2, 2.0555623}|\n",
      "|     40|{16, 4.2417116}|\n",
      "|     40|{19, 3.9465256}|\n",
      "|     40| {8, 3.9215052}|\n",
      "|     40| {2, 3.9037194}|\n",
      "|     40| {10, 3.762528}|\n",
      "|     40|{21, 3.5958273}|\n",
      "|     40|  {9, 3.067248}|\n",
      "|     40| {5, 3.0099266}|\n",
      "|     40| {4, 2.7588284}|\n",
      "|     40| {23, 2.628215}|\n",
      "+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alsModel.recommendForAllItems(10)\\\n",
    "    .selectExpr(\"movieId\", \"explode(recommendations)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.899920\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "    .setMetricName(\"rmse\")\\\n",
    "    .setLabelCol(\"rating\")\\\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = %f\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성과 평가지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀 평가지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "regComparison = predictions.select(\"rating\", \"prediction\")\\\n",
    "    .rdd.map(lambda x: (x(0), x(1)))\n",
    "metrics = RegressionMetrics(regComparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순위 평가지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics, RegressionMetrics\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "perUserActual = predictions\\\n",
    "    .where(\"rating > 2.5\")\\\n",
    "    .groupby(\"userId\")\\\n",
    "    .agg(expr(\"collect_set(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "perUserPredictions = predictions\\\n",
    "    .orderBy(col(\"userId\"), expr(\"prediction DESC\"))\\\n",
    "    .groupby(\"userId\")\\\n",
    "    .agg(expr(\"collect_list(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "perUserActualvPred = perUserActual.join(perUserPredictions, [\"userId\"])\\\n",
    "    .rdd.map(lambda row: (row[1], row[2][:15]))\n",
    "ranks = RankingMetrics(perUserActualvPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29497233535695083"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.meanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5846153846153845"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.precisionAt(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
