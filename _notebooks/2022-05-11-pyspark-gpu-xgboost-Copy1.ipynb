{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d691d78-6b44-4170-9bb2-6a005e0a6726",
   "metadata": {},
   "source": [
    "# \"[Spark] Spark GPU Cluster에서 XGBoost 분산 GPU 학습하기\"\n",
    "> Spark GPU Cluster에서 XGBoost GPU Regression 학습 테스트\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Spark]\n",
    "- tags: [spark, gpu, xgboost]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e69605-cb56-490b-be31-e391a85f322f",
   "metadata": {},
   "source": [
    "# Dependency\n",
    "- CentOS 7\n",
    "- Nvidia driver 510.73.05\n",
    "- Spark 3.2.1\n",
    "- Python 3.7\n",
    "- jars\n",
    "    - cudf-22.04.0-cuda11.jar\n",
    "    - rapids-4-spark_2.12-22.04.0.jar\n",
    "    - xgboost4j-spark_3.0-1.4.2-0.3.0.jar\n",
    "    - xgboost4j_3.0-1.4.2-0.3.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc97d4-cd67-4859-885d-985b51a2a3c0",
   "metadata": {},
   "source": [
    "# XGBoost4J-Spark-GPU 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a98982-a140-4775-be59-5780d1600da9",
   "metadata": {},
   "source": [
    "[spark-rapids github](https://github.com/NVIDIA/spark-rapids)을 참고하여 설정 하였다. 설정이 매우 간단한 거 같지만 dependency 문제 때문에 생각보다 시간이 많이 소요되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc87b22-69ca-421e-8ff5-252317a6555f",
   "metadata": {},
   "source": [
    "- 아래 jars들을 [Maven Repository](https://mvnrepository.com)에서 검색하여 `$SPARK_HOME/jars` 경로에 넣어 준다. 버전이 중요하다. 버전이 안 맞으면 온갖 exception에 시달린다.\n",
    "    - cudf-22.04.0-cuda11.jar\n",
    "    - rapids-4-spark_2.12-22.04.0.jar\n",
    "    - xgboost4j-spark_3.0-1.4.2-0.3.0.jar\n",
    "    - xgboost4j_3.0-1.4.2-0.3.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211a5c9-5064-40b7-bf57-729ec7695081",
   "metadata": {},
   "source": [
    "#### 아래는 GPU Resource가 있는 Spark Worker에서 진행\n",
    "- [Spark 공식 github](https://github.com/apache/spark/blob/master/examples/src/main/scripts/getGpusResources.sh) 에 접속하여 `getGpusResources.sh` 스크립트를 다운 받는다.\n",
    "- `getGpusResources.sh` 스크립트는 스파크가 GPU 자원이 있는지 인식할 수 있게 해준다.\n",
    "- `getGpusResources.sh`를 `$SPARK_HOME/drivers` 경로에 넣어 주고(`$SPARK_HOME/drivers`가 아니여도 상관없고, 디렉토리가 없으면 만들어서 넣어줌), 실행 권한을 준다.\n",
    "\n",
    "```sh\n",
    "$ chmod +x $SPARK_HOME/drivers/getGpusResources.sh\n",
    "```\n",
    "\n",
    "- GPU가 있는 Spark Worker의 `$SPARK_HOME/conf/spark-env.sh` 를 열어 Spark Worker가 `getGpusResources.sh` 스크립트 위치를 알 수 있도록 아래 내용을 넣어준다.\n",
    "\n",
    "```sh\n",
    "$ vi $SPARK_HOME/conf/spark-env.sh\n",
    "```\n",
    "\n",
    "```sh\n",
    "# $SPARK_HOME/conf/spark-env.sh\n",
    "export SPARK_WORKER_OPTS=\"-Dspark.worker.resource.gpu.amount=1 -Dspark.worker.resource.gpu.discoveryScript=/home/a/spark/drivers/getGpusResources.sh\"\n",
    "```\n",
    "\n",
    "- `$SPARK_HOME/conf/spark-defaults.conf` 를 열어 아래 내용을 추가한다.\n",
    "\n",
    "```sh\n",
    "spark.kryo.registrator  com.nvidia.spark.rapids.GpuKryoRegistrator\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11915c8b-1ec0-45b2-9481-ae8c76502fa1",
   "metadata": {},
   "source": [
    "# XGBoost 분산 GPU 학습 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f6db1c-a354-441c-a00b-034eb8330116",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09acd0ea-12ec-4af0-8858-561d82d216dd",
   "metadata": {},
   "source": [
    "## 관련 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b25d08aa-1888-488a-b6cc-2e27d93b4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setAppName(\"Spark-XGBoost-GPU\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "# conf.set(\"spark.jars\", \"s3://gourav-bucket/gourav/gpu/cudf-0.9.2.jar,s3://gourav-bucket/gourav/gpu/rapids-4-spark_2.12-0.1.0.jar,s3://gourav-bucket/gourav/gpu/cudf-0.14-cuda10-1.jar\")\n",
    "# conf.set(\"spark.executor.instances\", \"20\")\n",
    "# conf.set(\"spark.executor.cores\", \"4\")\n",
    "conf.set(\"spark.task.cpus\", \"2\")\n",
    "# conf.set(\"spark.executor.memory\", \"4g\")\n",
    "# conf.set(\"spark.executor.memoryOverhead\", \"2G\")\n",
    "conf.set(\"spark.executor.extraJavaOptions\", \"-Dai.rapids.cudf.prefer-pinned=true\")\n",
    "conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "conf.set(\"spark.task.resource.gpu.amount\", \"0.25\")\n",
    "conf.set(\"spark.locality.wait\", \"0s\")\n",
    "conf.set(\"spark.executor.resource.gpu.discoveryScript\", \"/home/a/spark/drivers/getGpusResources.sh\")\n",
    "conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "conf.set('spark.rapids.sql.enabled','true')\n",
    "conf.set('spark.rapids.sql.explain', 'ALL') # ALL/NONE/NOT_ON_GPU\n",
    "conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"3\")\n",
    "conf.set(\"spark.rapids.memory.pinnedPool.size\", \"1G\")\n",
    "# conf.set(\"spark.rapids.sql.hasNans\", \"false\")\n",
    "# conf.set('spark.rapids.sql.batchSizeBytes', '512M')\n",
    "# conf.set('spark.rapids.sql.reader.batchSizeBytes', '768M')\n",
    "# conf.set('spark.rapids.sql.variableFloatAgg.enabled', 'true')\n",
    "# conf.set('spark.rapids.sql.incompatibleDateFormats.enabled', 'true')\n",
    "# conf.set('spark.rapids.sql.udfCompiler.enabled', 'true')\n",
    "# conf.set('spark.rapids.sql.csv.read.double.enabled', 'true')\n",
    "# conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "conf.set(\"spark.sql.files.maxPartitionBytes\", \"512m\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .master(\"spark://lab100:7077\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.addPyFile(\"/home/a/spark/jars/rapids-4-spark_2.12-22.04.0.jar\")\n",
    "spark.sparkContext.addPyFile(\"/home/a/spark/jars/xgboost4j-spark_3.0-1.4.2-0.3.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcbba659-0ddf-45da-9073-3bb872b5c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MINIO_ACCESS_KEY = os.environ['MINIO_ACCESS_KEY']\n",
    "MINIO_SECRET_KEY = os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.endpoint\", \"http://lab101:10170\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"com.amazonaws.services.s3.enableV2\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6de0b489-cff3-4da1-b966-5d36b3648be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.dmlc.xgboost4j.scala.spark import XGBoostRegressionModel, XGBoostRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, IntegerType, StructField, StructType\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12857f-f36a-490f-9f55-e258ee6ca491",
   "metadata": {},
   "source": [
    "## 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3615b68-0c6e-4018-aa71-7beb3cfebeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca5b4dfe-db60-401c-b7a6-bc443e997dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'fare_amount'\n",
    "schema = StructType([\n",
    "    StructField('vendor_id', FloatType()),\n",
    "    StructField('passenger_count', FloatType()),\n",
    "    StructField('trip_distance', FloatType()),\n",
    "    StructField('pickup_longitude', FloatType()),\n",
    "    StructField('pickup_latitude', FloatType()),\n",
    "    StructField('rate_code', FloatType()),\n",
    "    StructField('store_and_fwd', FloatType()),\n",
    "    StructField('dropoff_longitude', FloatType()),\n",
    "    StructField('dropoff_latitude', FloatType()),\n",
    "    StructField(label, FloatType()),\n",
    "    StructField('hour', FloatType()),\n",
    "    StructField('year', IntegerType()),\n",
    "    StructField('month', IntegerType()),\n",
    "    StructField('day', FloatType()),\n",
    "    StructField('day_of_week', FloatType()),\n",
    "    StructField('is_weekend', FloatType()),\n",
    "])\n",
    "features = [ x.name for x in schema if x.name != label ]\n",
    "\n",
    "train_data = reader.schema(schema).option('header', True).csv('s3a://data/taxi-small/train')\n",
    "trans_data  = reader.schema(schema).option('header', True).csv('s3a://data/taxi-small/trainWithEval/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb5436-29ab-465f-9b86-f886e5199078",
   "metadata": {},
   "source": [
    "## XGBoostRegressor 생성\n",
    "- 처음에 공식문서가 아닌 구글링을 기반으로 테스트하다가 이 부분 때문에 고생했다. setFeaturesCols은 GPU 버전에서만 사용할 수 있고, CPU 버전에서는 setFeaturesCols이 없고 대신 setFeaturesCol을 써야 한다. CPU 버전에서는 하나의 벡터를 만들어서 넣어줘야 한다.\n",
    "- CPU 버전에서는 아래와 같이 전처리 한다.\n",
    "\n",
    "```python\n",
    "# CPU Version\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def vectorize(data_frame):\n",
    "    to_floats = [ col(x.name).cast(FloatType()) for x in data_frame.schema ]\n",
    "    return (VectorAssembler()\n",
    "        .setInputCols(features)\n",
    "        .setOutputCol('features')\n",
    "        .transform(data_frame.select(to_floats))\n",
    "        .select(col('features'), col(label)))\n",
    "\n",
    "train_data = vectorize(train_data)\n",
    "trans_data = vectorize(trans_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b54ad-dd05-4600-b5f1-cb3f275c1757",
   "metadata": {},
   "source": [
    "- parmas의 treeMethod에서 GPU Version에서는 `gpu_hist`만 지원한다. `hist`는 CPU Version에서만 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18c6cef6-9ed0-42ff-8a2c-e85df27b6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \n",
    "    'eta': 0.05,\n",
    "    'treeMethod': 'gpu_hist',\n",
    "    'maxDepth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'gamma': 1.0,\n",
    "    'numRound': 100,\n",
    "    'numWorkers': 1,\n",
    "}\n",
    "\n",
    "# GPU Version\n",
    "regressor = XGBoostRegressor(**params).setLabelCol(label).setFeaturesCols(features)\n",
    "\n",
    "# CPU Version\n",
    "# regressor = XGBoostRegressor(**params).setLabelCol(label).setFeaturesCol('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3fafb4-c719-479e-8eba-06c58ddfc4a8",
   "metadata": {},
   "source": [
    "## Train the Data with Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deb3b937-bac2-4ea9-a11b-d36d9f78071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training takes 10.44 seconds\n"
     ]
    }
   ],
   "source": [
    "def with_benchmark(phrase, action):\n",
    "    start = time()\n",
    "    result = action()\n",
    "    end = time()\n",
    "    print('{} takes {} seconds'.format(phrase, round(end - start, 2)))\n",
    "    return result\n",
    "model = with_benchmark('Training', lambda: regressor.fit(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a42c0b-850c-4ce8-bd7a-03f68f495fcb",
   "metadata": {},
   "source": [
    "## 모델 저장 및 재로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4151779-6be2-463e-8d84-0eb06cb47430",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save('s3a://data/new-model-path')\n",
    "loaded_model = XGBoostRegressionModel().load('s3a://data/new-model-path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc3ecc-1b86-47e3-a401-a9b8e711cd17",
   "metadata": {},
   "source": [
    "## 결과\n",
    "- CPU Version에서는 `select` function을 쓸 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "181c1c3c-d8f1-4970-a88a-1b2dd9ae2058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation takes 2.22 seconds\n",
      "+------------+---------------+-------------+-----------+-----------------+\n",
      "|   vendor_id|passenger_count|trip_distance|fare_amount|       prediction|\n",
      "+------------+---------------+-------------+-----------+-----------------+\n",
      "|1.55973043E9|            1.0|          1.5|        6.0|8.171152114868164|\n",
      "|1.55973043E9|            1.0|          2.1|        9.5|9.933568954467773|\n",
      "|1.55973043E9|            1.0|          1.7|        9.5|8.162859916687012|\n",
      "|1.55973043E9|            1.0|          0.8|        6.0|5.170645713806152|\n",
      "|1.55973043E9|            1.0|          0.8|        5.5|5.057125091552734|\n",
      "+------------+---------------+-------------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transform():\n",
    "    result = loaded_model.transform(trans_data).cache()\n",
    "    result.foreachPartition(lambda _: None)\n",
    "    return result\n",
    "result = with_benchmark('Transformation', transform)\n",
    "result.select('vendor_id', 'passenger_count', 'trip_distance', label, 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67fe847f-03d0-480a-ae38-153db44924cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation takes 0.27 seconds\n",
      "RMSE is 1.8364262858328744\n"
     ]
    }
   ],
   "source": [
    "accuracy = with_benchmark(\n",
    "    'Evaluation',\n",
    "    lambda: RegressionEvaluator().setLabelCol(label).evaluate(result))\n",
    "print('RMSE is ' + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
