{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"[Spark] PySpark Read Write\"\n",
    "> pyspark에 데이터 불러오고 내보내기\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Spark]\n",
    "- tags: [spark, pyspark, read, write]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MINIO_ACCESS_KEY = os.environ['MINIO_ACCESS_KEY']\n",
    "MINIO_SECRET_KEY = os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.endpoint\", \"http://lab101:10170\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"com.amazonaws.services.s3.enableV2\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3a://data/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\\\n",
    "    .save(\"s3a://tmp/my-tsv-file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON 파일\n",
    "- 자바스크립트 객체 표기법(JavaScript Object Notation)\n",
    "- 스파크에서는 JSON 파일을 사용할 때, 줄로 구분된 JSON을 기본적으로 사용\n",
    "- **multiLine** 옵션을 이용해 줄로 구분된 방식과 여러 줄로 구성된 방식을 선택적으로 사용할 수 있음\n",
    "- **multiLine** 옵션을 **true**로 설정하면 전체 파일을 하나의 JSON 객체로 읽을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3a://data/flight-data/json/2010-summary.json\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"json\").mode(\"overwrite\").save(\"s3a://tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet 파일\n",
    "- 읽기 연산시 JSON이나 CSV보다 효율적으로 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\")\\\n",
    "    .load(\"s3a://data/flight-data/parquet/2010-summary.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 읽기/쓰기 | 키 | 사용 가능한 값 | 기본값 | 설명 |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| 모두 | compression 또는 codec | none, uncompressed, bzip2, deflate, gzip, deflate, gzip, lz4, snappy | none | 스파크가 파일을 읽고 쓸 때 사용하는 압축 코덱 정의 |\n",
    "| 읽기 | mergeSchema | true, false | spark.sql.parquet.mergeSchema 속성의 설정 값 | 동일한 테이블이나 폴더에 신규 추가된 파케이 파일에 컬럼을 점진적으로 추가할 수 있음 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"s3a://tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORC 파일\n",
    "- 하둡 워크로드를 위해 설계된 self-describing이며, 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷\n",
    "- 대규모 스트리밍 읽기에 최적화\n",
    "- 필요한 로우를 신속하게 찾아낼 수 있는 기능이 통합\n",
    "- 파케이가 스파크에 최적화된 반면 ORC는 하이브에 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"orc\").load(\"s3a://data/flight-data/orc/2010-summary.orc\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"orc\").mode(\"overwrite\").save(\"s3a://tmp/my-orc-file.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                rows|\n",
      "+--------------------+\n",
      "|[DEST_COUNTRY_NAM...|\n",
      "|[United States, R...|\n",
      "|[United States, I...|\n",
      "|[United States, I...|\n",
      "|[Egypt, United St...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txtFile = spark.read.format(\"text\").load(\"s3a://data/flight-data/csv/2010-summary.csv\").selectExpr(\"split(value, ',') as rows\")\n",
    "txtFile.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----+----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|  _3|  _4|\n",
      "+-----------------+-------------------+-----+----+----+\n",
      "|    United States|            Romania|    1|null|null|\n",
      "|    United States|            Ireland|  264|null|null|\n",
      "|    United States|              India|   69|null|null|\n",
      "|            Egypt|      United States|   24|null|null|\n",
      "|Equatorial Guinea|      United States|    1|null|null|\n",
      "+-----------------+-------------------+-----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile_list = list(txtFile.toPandas()['rows'])\n",
    "csvFile_df = pd.DataFrame(csvFile_list[1:], columns = csvFile_list[:1][0] + [\"_3\", \"_4\"])\n",
    "csvFile = spark.createDataFrame(csvFile_df)\n",
    "csvFile.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 파일을 쓸 때는 문자열 컬럼이 하나만 존재해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"s3a://tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 파일에 데이터를 저장할 때 파티셔닝 작업을 수행하면 더 많은 컬럼을 저장 가능\n",
    "- 모든 파일에 컬럼을 추가하는 것이 아니라 텍스트 파일이 저장되는 디렉터리에 폴더 별로 컬름을 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\")\\\n",
    "    .write.partitionBy(\"count\").text(\"s3a://tmp/five-csv-files2py.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "scaleDF = spark.read.parquet(\"s3a://data/simple-ml-scaling\")\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "fittedPCA = pca.fit(scaleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Model Save\n",
    "fittedPCA.write().overwrite().save(\"s3a://tmp/fittedPCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------+\n",
      "| id|      features|PCAModel_06b47752a026__output|\n",
      "+---+--------------+-----------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|         [0.07137194992484...|\n",
      "|  1| [2.0,1.1,1.0]|         [-1.6804946984073...|\n",
      "|  0|[1.0,0.1,-1.0]|         [0.07137194992484...|\n",
      "|  1| [2.0,1.1,1.0]|         [-1.6804946984073...|\n",
      "|  1|[3.0,10.1,3.0]|         [-10.872398139848...|\n",
      "+---+--------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ML Model Load\n",
    "from pyspark.ml.feature import PCAModel\n",
    "\n",
    "loadedPCA = PCAModel.load(\"s3a://tmp/fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 고급 I/O 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 병렬로 데이터 쓰기\n",
    "- 데이터 파티션 당 하나의 파일이 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 안에 5개의 파일 생성\n",
    "csvFile.repartition(5).write.format(\"csv\").save(\"s3a://tmp/multiple.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파티셔닝\n",
    "- 파티셔닝은 어떤 데이터를 어디에 저장할 것인지 제어할 수 있는 기능\n",
    "- 데이터를 읽을 때 전체 데이터셋을 스캔하지 않고 필요한 컬럼의 데이터만 읽을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .save(\"s3a://tmp/partitioned-files.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 버켓팅\n",
    "- 각 파일에 저장된 데이터를 제어할 수 있는 또 다른 파일 조직화 기법\n",
    "- 버켓팅을 사용하면 동일한 버킷 ID를 가진 데이터가 하나의 물리적 파티션에 모두 모여 있기 때문에 데이터를 읽을 때 셔플을 피할 수 있음\n",
    "- 버켓팅은 스파크 관리 테이블에서만 사용 가능\n",
    "\n",
    "```python\n",
    "number_buckets = 10\n",
    "column_to_bucket_by = \"count\"\n",
    "\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    "    .bucketBy(number_buckets, column_to_bucket_by).saveAsTable(\"bucketFiles\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 크기 관리\n",
    "- 파일당 최대 5000개의 로우를 포함하도록 보장\n",
    "\n",
    "```python\n",
    "df.write.option(\"maxRecordPerFile\", 5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL\n",
    "\n",
    "```sh\n",
    "$ ./bin/spark-shell --driver-class-path mysql-connector-java-8.0.17.jar --jars mysql-connector-java-8.0.17.jar\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
