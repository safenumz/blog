{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"[Spark] PySpark Feature Engineering 기법들\"\n",
    "> pyspark feature engineering 기법들\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Spark]\n",
    "- tags: [spark, pyspark, feature, engineering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MINIO_ACCESS_KEY = os.environ['MINIO_ACCESS_KEY']\n",
    "MINIO_SECRET_KEY = os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.endpoint\", \"http://lab101:10170\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"com.amazonaws.services.s3.enableV2\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration()\\\n",
    "    .set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3a://data/retail-data/by-day/*.csv\")\\\n",
    "    .where(\"Description IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeIntDF = spark.read.parquet(\"s3a://data/simple-ml-integers\")\n",
    "simpleDF = spark.read.json(\"s3a://data/simple-ml\")\n",
    "scaleDF = spark.read.parquet(\"s3a://data/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세일즈 데이터셋은 여러번 엑세스할 것이므로 캐시를 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 변환자\n",
    "- 다양한 방법으로 원시 데이터를 변환시키는 함수\n",
    "- 하나의 변수를 두 개의 다른 변수로 변환하거나, 변수를 Double 타입 등으로 변환하는 기능\n",
    "- 예) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------+\n",
      "|         Description|Tokenizer_f8c5e492a92d__output|\n",
      "+--------------------+------------------------------+\n",
      "|  RABBIT NIGHT LIGHT|          [rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |          [doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|          [12, message, car...|\n",
      "|BLUE HARMONICA IN...|          [blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|          [gumball, coat, r...|\n",
      "+--------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer().setInputCol(\"Description\")\n",
    "tkn.transform(sales.select(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 전처리 추정자\n",
    "- 수행하려는 변환이 입력 컬럼에 대한 데이터 또는 정보로 초기화되어야 할 때 필요\n",
    "- 특정 입력 데이터에 따라 구성되는 변환자\n",
    "- 예) StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_f10c68da2d39__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "ss = StandardScaler().setInputCol(\"features\")\n",
    "ss.fit(scaleDF).transform(scaleDF).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 고수준 변환자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 RFormula\n",
    "- 원 핫 인코딩을 수행해 문자열로 지정된 범주화된 입력변수를 자동으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 SQL 변환자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          119|      62|   14452.0|\n",
      "|          440|     143|   16916.0|\n",
      "|          630|      72|   17633.0|\n",
      "|           34|       6|   14768.0|\n",
      "|         1542|      30|   13094.0|\n",
      "|          854|     117|   17884.0|\n",
      "|           97|      12|   16596.0|\n",
      "|          756|      67|   15145.0|\n",
      "|           83|      13|   16858.0|\n",
      "|           56|       4|   13160.0|\n",
      "|         8873|      80|   16656.0|\n",
      "|          241|      43|   16212.0|\n",
      "|          258|      23|   13142.0|\n",
      "|           67|      14|   13811.0|\n",
      "|         1145|      62|   16600.0|\n",
      "|          568|      87|   15898.0|\n",
      "|        37720|    2491|   15311.0|\n",
      "|         1467|      62|   18263.0|\n",
      "|         2006|      94|   16353.0|\n",
      "|         1486|     161|   17659.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "    .setStatement(\"\"\"\n",
    "        SELECT sum(Quantity), count(*), CustomerID\n",
    "        FROM __THIS__\n",
    "        GROUP BY CustomerID\n",
    "    \"\"\")\n",
    "\n",
    "basicTransformation.transform(sales).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 벡터 조합기\n",
    "- 모든 특징을 하나의 큰 벡터로 연결하여 추정자에 전달하는 기능을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "|   7|   8|   9|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fakeIntDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_7237f61f3237__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 연속형 특징 처리\n",
    "- 버켓팅이라는 프로세스를 통해 연속형 특징을 범주형 특징으로 변환\n",
    "- 여러 요구사항에 따라 특징을 스케일링하거나 정규화할 수 있음\n",
    "- Double 타입에서만 작동하므로 다른 형식의 숫자값이 있다면 Double Type으로 변경해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "| 0.0|\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "| 3.0|\n",
      "| 4.0|\n",
      "| 5.0|\n",
      "| 6.0|\n",
      "| 7.0|\n",
      "| 8.0|\n",
      "| 9.0|\n",
      "|10.0|\n",
      "|11.0|\n",
      "|12.0|\n",
      "|13.0|\n",
      "|14.0|\n",
      "|15.0|\n",
      "|16.0|\n",
      "|17.0|\n",
      "|18.0|\n",
      "|19.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "contDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 버켓팅\n",
    "- Bucketizer를 사용하면 주어진 연속형 특징을 지정한 버켓으로 분할\n",
    "- 예를 들어, 체중을 분석하고 싶을 때, 과체중, 평균, 저체중의 세 가지 버켓으로 나누어 활용하는 것이 간단한 접근이 될 수 있음\n",
    "- 경계 지정 규칙\n",
    "    - 분할 배열의 최소값은 DF의 최소값보다 작아야 함\n",
    "    - 분할 배열의 최대값은 DF의 최대값보다 커야 함\n",
    "    - 분할 배열은 최소 세 개 이상의 값을 지정해서 두 개 이상의 버켓을 만들도록 해야 함\n",
    "- 가능한 모든 범위를 포함하기 위해 **float(\"inf\"), float(\"-inf\")** 를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|  id|Bucketizer_e67baba75dea__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            0.0|\n",
      "| 2.0|                            0.0|\n",
      "| 3.0|                            0.0|\n",
      "| 4.0|                            0.0|\n",
      "| 5.0|                            1.0|\n",
      "| 6.0|                            1.0|\n",
      "| 7.0|                            1.0|\n",
      "| 8.0|                            1.0|\n",
      "| 9.0|                            1.0|\n",
      "|10.0|                            2.0|\n",
      "|11.0|                            2.0|\n",
      "|12.0|                            2.0|\n",
      "|13.0|                            2.0|\n",
      "|14.0|                            2.0|\n",
      "|15.0|                            2.0|\n",
      "|16.0|                            2.0|\n",
      "|17.0|                            2.0|\n",
      "|18.0|                            2.0|\n",
      "|19.0|                            2.0|\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기준을 직접 지정하지 않고 백분위수를 기준으로 분할하는 방법도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|result|\n",
      "+----+------+\n",
      "| 0.0|   0.0|\n",
      "| 1.0|   0.0|\n",
      "| 2.0|   0.0|\n",
      "| 3.0|   1.0|\n",
      "| 4.0|   1.0|\n",
      "| 5.0|   1.0|\n",
      "| 6.0|   1.0|\n",
      "| 7.0|   2.0|\n",
      "| 8.0|   2.0|\n",
      "| 9.0|   2.0|\n",
      "|10.0|   2.0|\n",
      "|11.0|   3.0|\n",
      "|12.0|   3.0|\n",
      "|13.0|   3.0|\n",
      "|14.0|   3.0|\n",
      "|15.0|   4.0|\n",
      "|16.0|   4.0|\n",
      "|17.0|   4.0|\n",
      "|18.0|   4.0|\n",
      "|19.0|   4.0|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\").setOutputCol(\"result\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 StandardScaler\n",
    "- 특징들이 평균이 0이고 표준편차가 1인 분포를 갖도록 데이터를 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_003a7eaa6e31__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 MinMaxScaler\n",
    "- 벡터의 값을 주어진 최솟값에서 최댓값까지의 비례 값으로 스케일링 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_7af6603d580f__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 MaxAbsScaler\n",
    "- 각 값을 해당 컬럼의 최대 절댓값으로 나눠서 데이터의 범위를 조정\n",
    "- 모든 값은 -1과 1사이에서 끝남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MaxAbsScaler_035b244c7f59__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                    [1.0,1.0,1.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "fittedmaScaler = maScaler.fit(scaleDF)\n",
    "fittedmaScaler.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 ElementwiseProduct\n",
    "- 벡터의 각 값을 임의의 값으로 조정할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|ElementwiseProduct_fd2f8252b564__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                      [30.0,151.5,60.0]|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    "    .setScalingVec(scaleUpVec)\\\n",
    "    .setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Normalizer\n",
    "- 여러 가지 표준 중 하나를 사용하여(파라미터 p로 지정) 다차원 벡터를 스케일링할 수 있음\n",
    "- 예를 들어, 맨해튼 표준은 p=1, 유클리드 표준은 p=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_3faba6829ca5__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 범주형 특징 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 StringIndexer\n",
    "- DF에 첨부된 메타데이터를 생성하여 어떤 입력이 어떤 출력에 해당하는지 지정, 이렇게 하면 나중에 각 색인값에서 입력값을 다시 가져올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문자열이 아닌 컬럼에도 StringIndexer를 적용할 수 있음, 색인을 생성하기 전에 문자열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     0.0|\n",
      "| blue| bad|     8|14.386294994851129|     7.0|\n",
      "| blue| bad|    12|14.386294994851129|     1.0|\n",
      "|green|good|    15| 38.97187133755819|     3.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    35|14.386294994851129|     5.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     4.0|\n",
      "|  red| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    45| 38.97187133755819|     6.0|\n",
      "|green|good|     1|14.386294994851129|     0.0|\n",
      "| blue| bad|     8|14.386294994851129|     7.0|\n",
      "| blue| bad|    12|14.386294994851129|     1.0|\n",
      "|green|good|    15| 38.97187133755819|     3.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     2.0|\n",
      "|  red|good|    35|14.386294994851129|     5.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     4.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습할 때 보지 못한 입력이 들어오면 기본적으로 오류가 발생함\n",
    "- 유효하지 않은 입력에 대해 오류를 출력하거나 로우를 건너뛰고 처리하는 옵션만 선택할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_fcfe830fcc9b, handleInvalid=skip"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valIndexer.setHandleInvalid(\"skip\")\n",
    "valIndexer.fit(simpleDF).setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 색인된 값을 텍스트로 변환\n",
    "- 다양하게 변환된 결과값을 기존 값으로 다시 매핑하여 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|color| lab|value1|            value2|labelInd|IndexToString_b45fc27c6240__output|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|green|good|     1|14.386294994851129|     1.0|                              good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                               bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                               bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                              good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                              good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                               bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                              good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                               bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                               bad|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|                               bad|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|                              good|\n",
      "|green|good|     1|14.386294994851129|     1.0|                              good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                               bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                               bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                              good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                              good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                               bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                              good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                               bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                               bad|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "labelReverse = IndexToString().setInputCol(\"labelInd\")\n",
    "labelReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 벡터 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     features|label|\n",
      "+-------------+-----+\n",
      "|[1.0,2.0,3.0]|    1|\n",
      "|[2.0,5.0,6.0]|    2|\n",
      "|[1.0,8.0,9.0]|    3|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "idxIn = spark.createDataFrame([\n",
    "    (Vectors.dense(1, 2, 3), 1),\n",
    "    (Vectors.dense(2, 5, 6), 2),\n",
    "    (Vectors.dense(1, 8, 9), 3)\n",
    "]).toDF(\"features\", \"label\")\n",
    "idxIn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|     features|label|        idxed|\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|    1|[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|[0.0,8.0,9.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indxr = VectorIndexer()\\\n",
    "    .setInputCol(\"features\")\\\n",
    "    .setOutputCol(\"idxed\")\\\n",
    "    .setMaxCategories(2)\n",
    "indxr.fit(idxIn).transform(idxIn).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------------------------------+\n",
      "|color|colorInd|OneHotEncoder_650f9bb5b001__output|\n",
      "+-----+--------+----------------------------------+\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "+-----+--------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe.fit(colorLab).transform(colorLab).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|color|colorInd|\n",
      "+-----+--------+\n",
      "|green|     1.0|\n",
      "| blue|     2.0|\n",
      "| blue|     2.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|green|     1.0|\n",
      "| blue|     2.0|\n",
      "| blue|     2.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colorLab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 텍스트 데이터 변환자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 텍스트 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, , water, transfer, tattoos]      |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, , doorstop, red]         |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RegexTokenizer를 이용하면 공백뿐만 아니라 정규 표현식을 이용한 Tokenizer를 만들 수 있음\n",
    "- Java의 정규 표현식<sup>RegEx</sup>구문을 준수해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "rt = RegexTokenizer()\\\n",
    "    .setInputCol(\"Description\")\\\n",
    "    .setOutputCol(\"DescOut\")\\\n",
    "    .setPattern(\" \")\\\n",
    "    .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 공백을 사용하는 것이 아니라 사전에 제시된 패턴에 매칭되는 값을 출력할 수 있음\n",
    "- **gaps** 파라미터를 **false**로 설정하여 이 작업을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------+\n",
      "|Description                        |DescOut           |\n",
      "+-----------------------------------+------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[ ,  ]            |\n",
      "|DOUGHNUT LIP GLOSS                 |[ ,  ,  ]         |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[ ,  ,  ,  ]      |\n",
      "|BLUE HARMONICA IN BOX              |[ ,  ,  ,  ]      |\n",
      "|GUMBALL COAT RACK                  |[ ,  ]            |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[ ,  ,  ,  ,  ]   |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[ ,  ,  ]         |\n",
      "|CAMOUFLAGE LED TORCH               |[ ,  ]            |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[ ,  ,  ,  ,  ]   |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[ ,  ,  ,  ]      |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[ ,  ,  ,  ]      |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[ ,  ,  ,  ]      |\n",
      "|ROSE CARAVAN DOORSTOP              |[ ,  ]            |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[ ,  ,  ,  ]      |\n",
      "|STORAGE TIN VINTAGE LEAF           |[ ,  ,  ]         |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[ ,  ,  ,  ,  ,  ]|\n",
      "|POPCORN HOLDER                     |[ ]               |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[ ,  ,  ,  ,  ,  ]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[ ,  ,  ,  ,  ]   |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[ ,  ,  ,  ,  ]   |\n",
      "+-----------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "rt = RegexTokenizer()\\\n",
    "    .setInputCol(\"Description\")\\\n",
    "    .setOutputCol(\"DescOut\")\\\n",
    "    .setPattern(\" \")\\\n",
    "    .setGaps(False)\\\n",
    "    .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 일반적인 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+-------------------------------------+\n",
      "|Description                    |DescOut                              |StopWordsRemover_52dbe61992eb__output|\n",
      "+-------------------------------+-------------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|[12, message, cards, envelopes]      |\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |[blue, harmonica, box]               |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |[gumball, coat, rack]                |\n",
      "+-------------------------------+-------------------------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "    .setStopWords(englishStopWords)\\\n",
    "    .setInputCol(\"DescOut\")\n",
    "stops.transform(tokenized).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 단어 조합 만들기\n",
    "- 단어 조합이란 기술적으로 길이가 **n**인 단어의 시퀀스, 즉 **n-gram**으로 간주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------+\n",
      "|DescOut                              |NGram_88f7dd6c0325__output           |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit, night, light]               |\n",
      "|[doughnut, lip, gloss]               |[doughnut, lip, gloss]               |\n",
      "|[12, message, cards, with, envelopes]|[12, message, cards, with, envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue, harmonica, in, box]           |\n",
      "|[gumball, coat, rack]                |[gumball, coat, rack]                |\n",
      "|[skulls, , water, transfer, tattoos] |[skulls, , water, transfer, tattoos] |\n",
      "|[feltcraft, girl, amelie, kit]       |[feltcraft, girl, amelie, kit]       |\n",
      "|[camouflage, led, torch]             |[camouflage, led, torch]             |\n",
      "|[white, skull, hot, water, bottle]   |[white, skull, hot, water, bottle]   |\n",
      "|[english, rose, hot, water, bottle]  |[english, rose, hot, water, bottle]  |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|DescOut                              |NGram_9932ae155e60__output                             |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit night, night light]                            |\n",
      "|[doughnut, lip, gloss]               |[doughnut lip, lip gloss]                              |\n",
      "|[12, message, cards, with, envelopes]|[12 message, message cards, cards with, with envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue harmonica, harmonica in, in box]                 |\n",
      "|[gumball, coat, rack]                |[gumball coat, coat rack]                              |\n",
      "|[skulls, , water, transfer, tattoos] |[skulls ,  water, water transfer, transfer tattoos]    |\n",
      "|[feltcraft, girl, amelie, kit]       |[feltcraft girl, girl amelie, amelie kit]              |\n",
      "|[camouflage, led, torch]             |[camouflage led, led torch]                            |\n",
      "|[white, skull, hot, water, bottle]   |[white skull, skull hot, hot water, water bottle]      |\n",
      "|[english, rose, hot, water, bottle]  |[english rose, rose hot, hot water, water bottle]      |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(10, False)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 단어를 숫자로 변환\n",
    "### CountVectorizer\n",
    "- Output: (총 어휘 크기, 어휘에 포함된 단어 색인, 특정 단어의 출현 빈도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "|Description                    |DescOut                              |countVec                                     |\n",
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |(500,[149,185,212],[1.0,1.0,1.0])            |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |(500,[462,463,492],[1.0,1.0,1.0])            |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|(500,[35,41,166],[1.0,1.0,1.0])              |\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |(500,[10,16,36,352],[1.0,1.0,1.0,1.0])       |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |(500,[228,280,408],[1.0,1.0,1.0])            |\n",
      "|SKULLS  WATER TRANSFER TATTOOS |[skulls, , water, transfer, tattoos] |(500,[11,40,133],[1.0,1.0,1.0])              |\n",
      "|FELTCRAFT GIRL AMELIE KIT      |[feltcraft, girl, amelie, kit]       |(500,[60,64,69],[1.0,1.0,1.0])               |\n",
      "|CAMOUFLAGE LED TORCH           |[camouflage, led, torch]             |(500,[264],[1.0])                            |\n",
      "|WHITE SKULL HOT WATER BOTTLE   |[white, skull, hot, water, bottle]   |(500,[15,34,39,40,118],[1.0,1.0,1.0,1.0,1.0])|\n",
      "|ENGLISH ROSE HOT WATER BOTTLE  |[english, rose, hot, water, bottle]  |(500,[34,39,40,46,169],[1.0,1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"countVec\")\\\n",
    "    .setVocabSize(500)\\\n",
    "    .setMinTF(1)\\\n",
    "    .setMinDF(2)\n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|DescOut                                |\n",
      "+---------------------------------------+\n",
      "|[red, star, card, holder]              |\n",
      "|[jumbo, bag, red, retrospot]           |\n",
      "|[red, diner, wall, clock]              |\n",
      "|[lunch, bag, red, retrospot]           |\n",
      "|[red, flock, love, heart, photo, frame]|\n",
      "|[red, woolly, hottie, white, heart.]   |\n",
      "|[jumbo, bag, red, retrospot]           |\n",
      "|[edwardian, parasol, red]              |\n",
      "|[jumbo, bag, red, retrospot]           |\n",
      "|[red, hanging, heart, t-light, holder] |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfIdfIn = tokenized\\\n",
    "    .where(\"array_contains(DescOut, 'red')\")\\\n",
    "    .select(\"DescOut\")\\\n",
    "    .limit(10)\n",
    "tfIdfIn.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+\n",
      "|DescOut                                 |TFOut                                                                   |IDFCol                                                                                             |\n",
      "+----------------------------------------+------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+\n",
      "|[wrap, red, apples]                     |(10000,[52,3405,9749],[1.0,1.0,1.0])                                    |(10000,[52,3405,9749],[0.0,1.2992829841302609,0.0])                                                |\n",
      "|[pack, of, 20, napkins, red, apples]    |(10000,[52,429,3405,4495,6547,9012],[1.0,1.0,1.0,1.0,1.0,1.0])          |(10000,[52,429,3405,4495,6547,9012],[0.0,1.2992829841302609,1.2992829841302609,0.0,0.0,0.0])       |\n",
      "|[red, diner, wall, clock]               |(10000,[52,5265,6404,8737],[1.0,1.0,1.0,1.0])                           |(10000,[52,5265,6404,8737],[0.0,0.0,0.0,1.2992829841302609])                                       |\n",
      "|[red, retrospot, charlotte, bag]        |(10000,[52,415,4175,8448],[1.0,1.0,1.0,1.0])                            |(10000,[52,415,4175,8448],[0.0,0.0,0.0,1.2992829841302609])                                        |\n",
      "|[red, baby, bunting]                    |(10000,[52,1348,7997],[1.0,1.0,1.0])                                    |(10000,[52,1348,7997],[0.0,0.0,0.0])                                                               |\n",
      "|[alarm, clock, bakelike, red]           |(10000,[52,4995,8737,9001],[1.0,1.0,1.0,1.0])                           |(10000,[52,4995,8737,9001],[0.0,0.0,1.2992829841302609,0.0])                                       |\n",
      "|[3, tier, cake, tin, red, and, cream]   |(10000,[52,1923,2891,3791,4206,7815,9042],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(10000,[52,1923,2891,3791,4206,7815,9042],[0.0,0.0,0.0,0.0,0.0,0.0,0.0])                           |\n",
      "|[set/20, red, retrospot, paper, napkins]|(10000,[52,429,6283,8448,8690],[1.0,1.0,1.0,1.0,1.0])                   |(10000,[52,429,6283,8448,8690],[0.0,1.2992829841302609,0.0,1.2992829841302609,1.0116009116784799]) |\n",
      "|[set/6, red, spotty, paper, plates]     |(10000,[52,2390,5712,6260,8690],[1.0,1.0,1.0,1.0,1.0])                  |(10000,[52,2390,5712,6260,8690],[0.0,0.0,1.2992829841302609,1.2992829841302609,1.0116009116784799])|\n",
      "|[set/6, red, spotty, paper, cups]       |(10000,[52,2572,5712,6260,8690],[1.0,1.0,1.0,1.0,1.0])                  |(10000,[52,2572,5712,6260,8690],[0.0,0.0,1.2992829841302609,1.2992829841302609,1.0116009116784799])|\n",
      "+----------------------------------------+------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "tf = HashingTF()\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"TFOut\")\\\n",
    "    .setNumFeatures(10000)\n",
    "idf = IDF()\\\n",
    "    .setInputCol(\"TFOut\")\\\n",
    "    .setOutputCol(\"IDFCol\")\\\n",
    "    .setMinDocFreq(2)\n",
    "\n",
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.06367666050791741,-0.047254581749439244,0.0387911431491375]\n",
      "\n",
      "Text: [I, with, Java, could, use, case, classes] => \n",
      "Vector: [0.010081456042826176,-0.08104146995382117,0.07273546925612857]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.05691406726837159,-0.005072242021560669,-0.059899924695491796]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.sql import Row\n",
    "\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I with Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "word2Vec = Word2Vec()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"result\")\\\n",
    "    .setVectorSize(3)\\\n",
    "    .setMinCount(0)\n",
    "\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 특징 조작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 주성분 분석\n",
    "- 주성분 분석<sup>Principal Components Analysis</sup>은 데이터의 가장 중요한 측면을 찾는 수학적 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_b3c7be1d1cac__output                  |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060155975]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------------+\n",
      "|            countVec|ChiSqSelector_c638e4b9988e__output|\n",
      "+--------------------+----------------------------------+\n",
      "|(500,[149,185,212...|                         (2,[],[])|\n",
      "|(500,[462,463,492...|                         (2,[],[])|\n",
      "|(500,[35,41,166],...|                         (2,[],[])|\n",
      "|(500,[10,16,36,35...|                         (2,[],[])|\n",
      "|(500,[228,280,408...|                         (2,[],[])|\n",
      "|(500,[11,40,133],...|                         (2,[],[])|\n",
      "|(500,[60,64,69],[...|                         (2,[],[])|\n",
      "|   (500,[264],[1.0])|                         (2,[],[])|\n",
      "|(500,[15,34,39,40...|                         (2,[],[])|\n",
      "|(500,[34,39,40,46...|                         (2,[],[])|\n",
      "|(500,[34,39,40,14...|                         (2,[],[])|\n",
      "|(500,[34,39,40,14...|                         (2,[],[])|\n",
      "|(500,[46,297],[1....|                         (2,[],[])|\n",
      "|(500,[3,4,11,143,...|                         (2,[],[])|\n",
      "|(500,[6,45,109,16...|                         (2,[],[])|\n",
      "|(500,[0,1,49,70,3...|               (2,[0,1],[1.0,1.0])|\n",
      "|(500,[21,296],[1....|                         (2,[],[])|\n",
      "|(500,[36,45,378],...|                         (2,[],[])|\n",
      "|(500,[2,6,328],[1...|                         (2,[],[])|\n",
      "|(500,[0,2,6,328,4...|                     (2,[0],[1.0])|\n",
      "+--------------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "\n",
    "tokenized = tkn\\\n",
    "    .transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    "    .where(\"CustomerId IS NOT NULL\")\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    "    .where(\"CustomerId IS NOT NULL\")\n",
    "chisq = ChiSqSelector()\\\n",
    "    .setFeaturesCol(\"countVec\")\\\n",
    "    .setLabelCol(\"CustomerId\")\\\n",
    "    .setNumTopFeatures(2)\n",
    "chisq.fit(prechi).transform(prechi)\\\n",
    "    .drop(\"customerId\", \"Description\", \"DescOut\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
